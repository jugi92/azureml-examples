{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install dependencies\n",
        "Before starting off, if you are running the notebook on Azure Machine Learning Studio or running first time locally, you will need the following packages"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install azure-ai-ml==1.23.1\n",
        "! pip install azure-identity==1.19.0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Connect to Azure Machine Learning workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "We are using `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [set up authentication doc](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk), [azure-identity reference doc](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import mltable\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "ml_client = MLClient.from_config(credential)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258666864
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Process exports\n",
        "The Azure Machine Learning feature \"Data Labeling\" can be used to label data and export the created labels.  \n",
        "This process needs to be done manually for now in each project under with the following settings:  \n",
        "Asset type: Labeled  \n",
        "Export Format: Azure ML dataset  \n",
        "Include these details in the export output: Labeler details\n",
        "\n",
        "After the export successfully ran, the code below will search for the latest versions of the exported datasets and combine those.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labeling_projects_to_combine = [\"project_based_on_data_from_notebook\", \"eco_labeling_1\"]\n",
        "new_dataset_name = \"combined_dataset\"\n",
        "new_dataset_description = \"This is the combined dataset of the projects: \" + \", \".join(labeling_projects_to_combine)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258667078
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "projects_with_datasets = {}\n",
        "for p in labeling_projects_to_combine:\n",
        "    projects_with_datasets[p] = []\n",
        "\n",
        "\n",
        "for d in ml_client.data.list():\n",
        "    for p in labeling_projects_to_combine:\n",
        "        if p in d.name:\n",
        "            projects_with_datasets[p].append(d)\n",
        "\n",
        "# get latest export per project\n",
        "for p, datasets in projects_with_datasets.items():\n",
        "    projects_with_datasets[p] = sorted(datasets, key=lambda x: x.name, reverse=True)[0]\n",
        "\n",
        "projects_with_datasets"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258667247
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_df = []\n",
        "for p, dataset in projects_with_datasets.items():\n",
        "    data_asset = ml_client.data.get(dataset.name, version=\"1\")\n",
        "    tbl = mltable.load(f'azureml:/{data_asset.id}')\n",
        "    df = tbl.to_pandas_dataframe()\n",
        "    list_of_df.append(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670116
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat(list_of_df)\n",
        "df.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670268
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# will format the streaminfo object of the MLTable to back to a string like:\n",
        "# azureml://subscriptions/5121ed4d-cbd2-4f4d-a566-99571d08db8f/resourcegroups/eco-azureml-rg/workspaces/eco-azureml/datastores/workspaceblobstore/paths/odFridgeObjects/images/85.jpg\n",
        "\n",
        "df.image_url = df.image_url.apply(\n",
        "    lambda x: \"azureml://subscriptions/\" +\n",
        "    x.arguments[\"subscription\"] + \n",
        "    \"/resourcegroups/\" + x.arguments[\"resourceGroup\"] +\n",
        "    \"/workspaces/\" + x.arguments[\"workspaceName\"] +\n",
        "    \"/datastores/\" + x.arguments[\"datastoreName\"] +\n",
        "    \"/paths\" + x.resource_id.lstrip(x.arguments[\"datastoreName\"])\n",
        "    )\n",
        "\n",
        "df.image_url = df.image_url.astype(str)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670407
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optionally filter on subset of labels\n",
        "labels_subset = [\"beverage\"]\n",
        "\n",
        "def filter_labels(row, labels_subset):\n",
        "    filtered_labels = [d for d in row['label'] if d['label'] in labels_subset]\n",
        "    filtered_confidence = [row['label_confidence'][i] for i, d in enumerate(row['label']) if d['label'] in labels_subset]\n",
        "    return pd.Series([filtered_labels, filtered_confidence])\n",
        "\n",
        "\n",
        "if labels_subset:\n",
        "    new_dataset_description = new_dataset_description + \" filtered for the following labels: \" + \", \".join(labels_subset)\n",
        "    df[['label', 'label_confidence']] = df.apply(filter_labels, labels_subset=labels_subset, axis=1)\n",
        "\n",
        "df.label.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670561
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.label_confidence.head(5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670720
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder = new_dataset_name\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "filename = \"labeledDatapoints_1.jsonl\"\n",
        "filename_with_folder = folder + \"/\" + filename\n",
        "\n",
        "with open(filename_with_folder, \"w\") as f:\n",
        "    f.write(df.to_json(orient='records', lines=True, force_ascii=False).replace(\"\\\\/\",\"/\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258670920
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create paths to the data files\n",
        "paths = [{\"file\": filename_with_folder}]\n",
        "\n",
        "# create an MLTable from the data files\n",
        "tbl = mltable.from_json_lines_files(\n",
        "    paths=paths\n",
        ")\n",
        "\n",
        "tbl = tbl.convert_column_types({\"image_url\": \"stream_info\"})\n",
        "\n",
        "tbl.save(folder)\n",
        "\n",
        "# Define the Data asset object\n",
        "my_data = Data(\n",
        "    path=folder,\n",
        "    type=AssetTypes.MLTABLE,\n",
        "    description=new_dataset_description,\n",
        "    name=new_dataset_name\n",
        ")\n",
        "\n",
        "# Create the data asset in the workspace\n",
        "ml_client.data.create_or_update(my_data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1736258681430
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
